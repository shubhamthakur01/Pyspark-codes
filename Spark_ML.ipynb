{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a9ab3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:40:14.432754Z",
     "start_time": "2022-03-08T20:40:14.427401Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "#from user_definition import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57defc26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:40:36.490687Z",
     "start_time": "2022-03-08T20:40:16.655621Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/08 12:40:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/08 12:40:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/03/08 12:40:35 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/03/08 12:40:35 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/03/08 12:40:35 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/03/08 12:40:35 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/03/08 12:40:35 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/03/08 12:40:35 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "sc = ss.sparkContext\n",
    "\n",
    "#if you have to create appName\n",
    "# ss = SparkSession.builder.appName(app_name).getOrCreate()\n",
    "# sc = ss.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5cca33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:40:36.495896Z",
     "start_time": "2022-03-08T20:40:36.492283Z"
    }
   },
   "outputs": [],
   "source": [
    "def FloatSafe(value): # In case there are non-integer type to be converted.\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def IntSafe(value): # In case there are non-integer type to be converted.\n",
    "    try:\n",
    "        return int(value)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fed334c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:41:16.574853Z",
     "start_time": "2022-03-08T20:41:15.660118Z"
    }
   },
   "outputs": [],
   "source": [
    "train_rdd = sc.textFile('train.csv', 4)\\\n",
    "            .map(lambda x:  x.split(\",\")).\\\n",
    "            map(lambda x : (FloatSafe(x[0]), FloatSafe(x[1]),FloatSafe(x[2]),IntSafe(x[3])  ) )\n",
    "\n",
    "test_rdd = sc.textFile('test.csv', 4)\\\n",
    "            .map(lambda x:  x.split(\",\")).\\\n",
    "            map(lambda x : (FloatSafe(x[0]), FloatSafe(x[1]),FloatSafe(x[2]),IntSafe(x[3])  ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fdb8df4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:42:16.701512Z",
     "start_time": "2022-03-08T20:42:16.696981Z"
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"perimeter\",DoubleType(),True),\n",
    "    StructField(\"length\",DoubleType(),True),\n",
    "    StructField(\"width\",DoubleType(),True),\n",
    "    StructField(\"label\",IntegerType(),True)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d15ccdda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:42:20.744492Z",
     "start_time": "2022-03-08T20:42:17.570915Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = ss.createDataFrame(train_rdd, schema)\n",
    "df_test = ss.createDataFrame(test_rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05d1c5b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:42:22.127044Z",
     "start_time": "2022-03-08T20:42:22.062863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- perimeter: double (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- width: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7c831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:42:47.015477Z",
     "start_time": "2022-03-08T20:42:47.012347Z"
    }
   },
   "source": [
    "# Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4734d02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:42:50.447013Z",
     "start_time": "2022-03-08T20:42:49.910819Z"
    }
   },
   "outputs": [],
   "source": [
    "va = VectorAssembler(outputCol=\"features\", inputCols=df_train.columns[0:-1]) #except the last col.\n",
    "df_train_va = va.transform(df_train).select(\"features\", \"label\")\n",
    "df_test_va = va.transform(df_test).select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8924b",
   "metadata": {},
   "source": [
    "# Standard Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "310699fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:43:20.549699Z",
     "start_time": "2022-03-08T20:43:15.038851Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "std = StandardScaler(inputCol=\"features\", outputCol=\"features_out\", withMean=True, withStd=True).fit(df_train_va)\n",
    "#df_train_std= std.fit_transform(df_train_va)\n",
    "df_test_std=std.transform(df_test_va).select('features_out','label').withColumnRenamed('features_out', 'features')\n",
    "df_train_std=std.transform(df_train_va).select('features_out','label').withColumnRenamed('features_out', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b317c",
   "metadata": {},
   "source": [
    "# String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8abac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "c = \"label\"\n",
    "si = StringIndexer(inputCol=c, outputCol=c+\"-num\") \n",
    "sm = si.fit(df_va) \n",
    "\n",
    "newdf = sm.transform(df_va).drop(c) \n",
    "newdf = newdf.withColumnRenamed(c+\"-num\", c)  \n",
    "\n",
    "newdf_test = sm.transform(df_va_test).drop(c) \n",
    "newdf_test = newdf_test.withColumnRenamed(c+\"-num\", c)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ab503",
   "metadata": {},
   "source": [
    "# Building Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "233e83d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:45:06.245618Z",
     "start_time": "2022-03-08T20:45:06.228332Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb4ce6",
   "metadata": {},
   "source": [
    "# Pipeline Building with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[va,dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a8569",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "347afc6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:46:08.911818Z",
     "start_time": "2022-03-08T20:45:48.011613Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator()#.metrics.setMetricName() \n",
    "paramGrid = ParamGridBuilder().addGrid(rf.maxDepth, [2,3,4,5]).build()\n",
    "cv = CrossValidator(estimator=rf, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=2, \n",
    "                    estimatorParamMaps=paramGrid)\n",
    "\n",
    "cvmodel = cv.fit(df_train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1a0d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:47:34.712222Z",
     "start_time": "2022-03-08T20:47:34.707405Z"
    }
   },
   "source": [
    "# Prediction using Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b07150aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:47:03.605780Z",
     "start_time": "2022-03-08T20:47:02.997400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8508974358974359"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtpredicts = cvmodel.bestModel.transform(df_test_std)\n",
    "metrics = MulticlassClassificationEvaluator()\\\n",
    "                .setLabelCol(\"label\")\\\n",
    "                .setPredictionCol(\"prediction\")\n",
    "\n",
    "\n",
    "#metrics.setMetricName(\"f1\") \n",
    "\n",
    "metrics.evaluate(dtpredicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e00fd",
   "metadata": {},
   "source": [
    "# Getting the Best Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "107f1f34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:48:42.114598Z",
     "start_time": "2022-03-08T20:48:42.108851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel.getMaxDepth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa7345",
   "metadata": {},
   "source": [
    "# Plain RandomForest Without Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2eeb0d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:51:07.603159Z",
     "start_time": "2022-03-08T20:51:05.898551Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(maxDepth=1,numTrees=2,maxBins=2,seed=6)\n",
    "rfmodel = rf.fit(df_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90bf5503",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T20:51:13.804221Z",
     "start_time": "2022-03-08T20:51:13.265541Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6825332562174667"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfpredicts = rfmodel.transform(df_test_std)\n",
    "metric_name = \"f1\"\n",
    "metrics = MulticlassClassificationEvaluator()\\\n",
    "                .setLabelCol(\"label\")\\\n",
    "                .setPredictionCol(\"prediction\")\n",
    "metrics.setMetricName(metric_name) \n",
    "\n",
    "metrics.evaluate(rfpredicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae28c01",
   "metadata": {},
   "source": [
    "# Spilit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d234f23a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T21:16:48.809302Z",
     "start_time": "2022-03-08T21:16:48.423981Z"
    }
   },
   "outputs": [],
   "source": [
    "pendtsets = df_train_std.randomSplit([0.8, 0.2], 1)\n",
    "df_std_train = pendtsets[0].cache()\n",
    "df_std_valid = pendtsets[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "369b887b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T21:21:12.876305Z",
     "start_time": "2022-03-08T21:21:12.501486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[-1.1636812938097...|    3|[0.0,0.5965346534...|[0.0,0.2982673267...|       3.0|\n",
      "|[-1.1019210378260...|    3|[0.0,0.5965346534...|[0.0,0.2982673267...|       3.0|\n",
      "|[-1.1482412298138...|    3|[0.0,0.5965346534...|[0.0,0.2982673267...|       3.0|\n",
      "|[-0.9089202378771...|    3|[0.0,0.5965346534...|[0.0,0.2982673267...|       3.0|\n",
      "|[-1.0478808138403...|    3|[0.0,0.5965346534...|[0.0,0.2982673267...|       3.0|\n",
      "|[-0.8008397899057...|    1|[0.0,0.5965346534...|[0.0,0.2982673267...|       3.0|\n",
      "|[-0.8162798539016...|    3|[0.0,0.5965346534...|[0.0,0.2982673267...|       3.0|\n",
      "|[-1.0092806538505...|    3|[0.0,0.5965346534...|[0.0,0.2982673267...|       3.0|\n",
      "|[-0.7004793739322...|    3|[0.0,0.5965346534...|[0.0,0.2982673267...|       3.0|\n",
      "|[-0.9012002058791...|    3|[0.0,0.5965346534...|[0.0,0.2982673267...|       3.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfpredicts.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bba71e",
   "metadata": {},
   "source": [
    "# MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0672dbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T21:25:23.624191Z",
     "start_time": "2022-03-08T21:25:22.902831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.625\n",
      "Recall = 1.0\n",
      "F1 Score = 0.7692307692307693\n",
      "Confusion Metrics = \n",
      "DenseMatrix([[ 4.,  4.,  6.],\n",
      "             [ 1., 15.,  0.],\n",
      "             [ 0.,  0., 10.]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "#prediction and label\n",
    "prediction_label = rfpredicts.select(\"prediction\", \"label\").rdd\n",
    "prediction_label = prediction_label.map(lambda x: (x[0], float(x[1]) )) # Intermediate\n",
    "\n",
    "\n",
    "metrics = MulticlassMetrics(prediction_label)\n",
    "\n",
    "precision = metrics.precision(3.0)\n",
    "recall = metrics.recall(3.0)\n",
    "f1Score = metrics.fMeasure(3.0)\n",
    "confusionMetrics = metrics.confusionMatrix()\n",
    "\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)\n",
    "print(\"Confusion Metrics = \\n%s\" % confusionMetrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad58686",
   "metadata": {},
   "source": [
    "# Building Kmeans Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06783821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans(k = 10, maxIter= 200, tol=0.1)\n",
    "model = kmeans.fit(df_train_std)\n",
    "\n",
    "center = model.clusterCenters()\n",
    "for c in center:\n",
    "    print(c)\n",
    "    \n",
    "prediction = model.transform(penlpoints)\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "sihoutte = evaluator.evaluate(prediction)\n",
    "sihoutte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e09e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T21:53:23.057734Z",
     "start_time": "2022-03-08T21:53:23.048781Z"
    }
   },
   "source": [
    "# String Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting strings to numeric values\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def indexStringColumns(df, cols):\n",
    "    # variable newdf will be updated several times\n",
    "    newdf = df\n",
    "    \n",
    "    for c in cols:\n",
    "        # For each given colum, fits StringIndexerModel.\n",
    "        si = StringIndexer(inputCol=c, outputCol=c+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        \n",
    "        # Creates a DataFame by putting the transformed values in the new colum with suffix \"-num\" \n",
    "        # and then drops the original columns.\n",
    "        # and drop the \"-num\" suffix. \n",
    "        newdf = sm.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-num\", c)\n",
    "    return newdf\n",
    "\n",
    "dfnumeric = indexStringColumns(dfrawnona, [\"workclass\", \"education\",\n",
    "                                           \"marital_status\", \"occupation\",\n",
    "                                           \"relationship\", \"race\", \"sex\", \n",
    "                                           \"native_country\", \"income\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd9c89",
   "metadata": {},
   "source": [
    "# One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d6bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "def oneHotEncodeColumns(df, cols):\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        ohe = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        ohe_model = ohe.fit(newdf)\n",
    "\n",
    "        newdf = ohe_model.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf\n",
    "\n",
    "dfhot = oneHotEncodeColumns(dfnumeric, [\"workclass\", \"education\", \n",
    "                                        \"marital_status\", \"occupation\", \n",
    "                                        \"relationship\", \"race\", \"native_country\"])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea8f24",
   "metadata": {},
   "source": [
    "# Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23606dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data with Vector Assembler.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "input_cols=[\"age\",\"capital_gain\",\"capital_loss\",\"fnlwgt\",\"hours_per_week\",\"sex\",\"workclass\",\n",
    "            \"education\",\"marital_status\",\"occupation\",\"relationship\",\"native_country\",\"race\"]\n",
    "\n",
    "#VectorAssembler takes a number of collumn names(inputCols) and output column name (outputCol)\n",
    "#and transforms a DataFrame to assemble the values in inputCols into one single vector with outputCol.\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=input_cols)\n",
    "#lpoints - labeled data.\n",
    "lpoints = va.transform(dfhot).select(\"features\", \"income\").withColumnRenamed(\"income\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf8e25",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model.\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(regParam=0.01, maxIter=1000, fitIntercept=True)\n",
    "lrmodel = lr.fit(adulttrain)\n",
    "#The above lines are same as..\n",
    "#lr = LogisticRegression()\n",
    "#lrmodel = lr.setParams(regParam=0.01, maxIter=1000, fitIntercept=True).fit(adulttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b993c",
   "metadata": {},
   "source": [
    "# LR Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668d52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpret the model parameters\n",
    "print(lrmodel.coefficients)\n",
    "print(lrmodel.intercept)\n",
    "\n",
    "validpredicts = lrmodel.transform(adultvalid)\n",
    "validpredicts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d93157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model. default metric : Area Under ROC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bceval = BinaryClassificationEvaluator()\n",
    "print (bceval.getMetricName() +\":\" + str(bceval.evaluate(validpredicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e37fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model. metric : Area Under PR\n",
    "bceval.setMetricName(\"areaUnderPR\")\n",
    "print (bceval.getMetricName() +\":\" + str(bceval.evaluate(validpredicts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c4848",
   "metadata": {},
   "source": [
    "# Cross Validation on LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cccc668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "cv = CrossValidator().setEstimator(lr).setEvaluator(bceval).setNumFolds(5)\n",
    "#ParamGridBuilder() â€“ combinations of parameters and their values.\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [1000])\\\n",
    ".addGrid(lr.regParam, [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]).build()\n",
    "#setEstimatorParamMaps() takes ParamGridBuilder().\n",
    "cv.setEstimatorParamMaps(paramGrid)\n",
    "cvmodel = cv.fit(adulttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvmodel.bestModel.coefficients)\n",
    "print(cvmodel.bestModel.intercept)\n",
    "print(cvmodel.bestModel.getMaxIter())\n",
    "print(cvmodel.bestModel.getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2907020",
   "metadata": {},
   "outputs": [],
   "source": [
    "BinaryClassificationEvaluator().evaluate(cvmodel.bestModel.transform(adultvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7730aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "BinaryClassificationEvaluator().setMetricName(\"areaUnderPR\").evaluate(cvmodel.bestModel.transform(adultvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47270a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\").evaluate(cvmodel.bestModel.transform(adultvalid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
